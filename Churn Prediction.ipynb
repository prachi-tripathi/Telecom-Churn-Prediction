{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Telecom_customer churn_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_Mean</th>\n",
       "      <th>mou_Mean</th>\n",
       "      <th>totmrc_Mean</th>\n",
       "      <th>da_Mean</th>\n",
       "      <th>ovrmou_Mean</th>\n",
       "      <th>ovrrev_Mean</th>\n",
       "      <th>vceovr_Mean</th>\n",
       "      <th>datovr_Mean</th>\n",
       "      <th>roam_Mean</th>\n",
       "      <th>change_mou</th>\n",
       "      <th>...</th>\n",
       "      <th>forgntvl</th>\n",
       "      <th>ethnic</th>\n",
       "      <th>kid0_2</th>\n",
       "      <th>kid3_5</th>\n",
       "      <th>kid6_10</th>\n",
       "      <th>kid11_15</th>\n",
       "      <th>kid16_17</th>\n",
       "      <th>creditcd</th>\n",
       "      <th>eqpdays</th>\n",
       "      <th>Customer_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.9975</td>\n",
       "      <td>219.25</td>\n",
       "      <td>22.500</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-157.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>361.0</td>\n",
       "      <td>1000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.4925</td>\n",
       "      <td>482.75</td>\n",
       "      <td>37.425</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>22.75</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Z</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>240.0</td>\n",
       "      <td>1000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.9900</td>\n",
       "      <td>10.25</td>\n",
       "      <td>16.990</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>1504.0</td>\n",
       "      <td>1000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0000</td>\n",
       "      <td>7.50</td>\n",
       "      <td>38.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>1812.0</td>\n",
       "      <td>1000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.2300</td>\n",
       "      <td>570.50</td>\n",
       "      <td>71.980</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.50</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>Y</td>\n",
       "      <td>434.0</td>\n",
       "      <td>1000005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_Mean  mou_Mean  totmrc_Mean  da_Mean  ovrmou_Mean  ovrrev_Mean  \\\n",
       "0   23.9975    219.25       22.500   0.2475         0.00          0.0   \n",
       "1   57.4925    482.75       37.425   0.2475        22.75          9.1   \n",
       "2   16.9900     10.25       16.990   0.0000         0.00          0.0   \n",
       "3   38.0000      7.50       38.000   0.0000         0.00          0.0   \n",
       "4   55.2300    570.50       71.980   0.0000         0.00          0.0   \n",
       "\n",
       "   vceovr_Mean  datovr_Mean  roam_Mean  change_mou  ...  forgntvl  ethnic  \\\n",
       "0          0.0          0.0        0.0     -157.25  ...       0.0       N   \n",
       "1          9.1          0.0        0.0      532.25  ...       0.0       Z   \n",
       "2          0.0          0.0        0.0       -4.25  ...       0.0       N   \n",
       "3          0.0          0.0        0.0       -1.50  ...       0.0       U   \n",
       "4          0.0          0.0        0.0       38.50  ...       0.0       I   \n",
       "\n",
       "   kid0_2  kid3_5  kid6_10  kid11_15  kid16_17  creditcd  eqpdays  Customer_ID  \n",
       "0       U       U        U         U         U         Y    361.0      1000001  \n",
       "1       U       U        U         U         U         Y    240.0      1000002  \n",
       "2       U       Y        U         U         U         Y   1504.0      1000003  \n",
       "3       Y       U        U         U         U         Y   1812.0      1000004  \n",
       "4       U       U        U         U         U         Y    434.0      1000005  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preliminary look at the imported dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rev_Mean', 'mou_Mean', 'totmrc_Mean', 'da_Mean', 'ovrmou_Mean',\n",
       "       'ovrrev_Mean', 'vceovr_Mean', 'datovr_Mean', 'roam_Mean', 'change_mou',\n",
       "       'change_rev', 'drop_vce_Mean', 'drop_dat_Mean', 'blck_vce_Mean',\n",
       "       'blck_dat_Mean', 'unan_vce_Mean', 'unan_dat_Mean', 'plcd_vce_Mean',\n",
       "       'plcd_dat_Mean', 'recv_vce_Mean', 'recv_sms_Mean', 'comp_vce_Mean',\n",
       "       'comp_dat_Mean', 'custcare_Mean', 'ccrndmou_Mean', 'cc_mou_Mean',\n",
       "       'inonemin_Mean', 'threeway_Mean', 'mou_cvce_Mean', 'mou_cdat_Mean',\n",
       "       'mou_rvce_Mean', 'owylis_vce_Mean', 'mouowylisv_Mean',\n",
       "       'iwylis_vce_Mean', 'mouiwylisv_Mean', 'peak_vce_Mean', 'peak_dat_Mean',\n",
       "       'mou_peav_Mean', 'mou_pead_Mean', 'opk_vce_Mean', 'opk_dat_Mean',\n",
       "       'mou_opkv_Mean', 'mou_opkd_Mean', 'drop_blk_Mean', 'attempt_Mean',\n",
       "       'complete_Mean', 'callfwdv_Mean', 'callwait_Mean', 'churn', 'months',\n",
       "       'uniqsubs', 'actvsubs', 'new_cell', 'crclscod', 'asl_flag', 'totcalls',\n",
       "       'totmou', 'totrev', 'adjrev', 'adjmou', 'adjqty', 'avgrev', 'avgmou',\n",
       "       'avgqty', 'avg3mou', 'avg3qty', 'avg3rev', 'avg6mou', 'avg6qty',\n",
       "       'avg6rev', 'prizm_social_one', 'area', 'dualband', 'refurb_new',\n",
       "       'hnd_price', 'phones', 'models', 'hnd_webcap', 'truck', 'rv', 'ownrent',\n",
       "       'lor', 'dwlltype', 'marital', 'adults', 'infobase', 'income',\n",
       "       'numbcars', 'HHstatin', 'dwllsize', 'forgntvl', 'ethnic', 'kid0_2',\n",
       "       'kid3_5', 'kid6_10', 'kid11_15', 'kid16_17', 'creditcd', 'eqpdays',\n",
       "       'Customer_ID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column Names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_df.shape\n",
    "# We see 100000 rows and 100 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rev_Mean      mou_Mean   totmrc_Mean       da_Mean   ovrmou_Mean  \\\n",
      "count   99643.000000  99643.000000  99643.000000  99643.000000  99643.000000   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean       58.719985    513.559937     46.179136      0.888828     41.072247   \n",
      "std        46.291677    525.168140     23.623489      2.177619     97.296150   \n",
      "min        -6.167500      0.000000    -26.915000      0.000000      0.000000   \n",
      "25%        33.260000    150.750000     30.000000      0.000000      0.000000   \n",
      "50%        48.195000    355.500000     44.990000      0.247500      2.750000   \n",
      "75%        70.750000    703.000000     59.990000      0.990000     42.000000   \n",
      "max      3843.262500  12206.750000    409.990000    159.390000   4320.750000   \n",
      "\n",
      "         ovrrev_Mean   vceovr_Mean   datovr_Mean     roam_Mean    change_mou  \\\n",
      "count   99643.000000  99643.000000  99643.000000  99643.000000  99109.000000   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean       13.559560     13.295062      0.261318      1.286405    -13.933818   \n",
      "std        30.500885     30.056089      3.126531     14.711374    276.087509   \n",
      "min         0.000000      0.000000      0.000000      0.000000  -3875.000000   \n",
      "25%         0.000000      0.000000      0.000000      0.000000    -87.000000   \n",
      "50%         1.000000      0.682500      0.000000      0.000000     -6.250000   \n",
      "75%        14.437500     14.025000      0.000000      0.235000     63.000000   \n",
      "max      1102.400000    896.087500    423.540000   3685.200000  31219.250000   \n",
      "\n",
      "          change_rev  drop_vce_Mean  drop_dat_Mean  blck_vce_Mean  \\\n",
      "count   99109.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique           NaN            NaN            NaN            NaN   \n",
      "top              NaN            NaN            NaN            NaN   \n",
      "freq             NaN            NaN            NaN            NaN   \n",
      "mean       -1.021067       5.955085       0.040520       4.022917   \n",
      "std        50.363209       8.954715       0.877136      10.672202   \n",
      "min     -1107.740000       0.000000       0.000000       0.000000   \n",
      "25%        -7.365000       0.666667       0.000000       0.000000   \n",
      "50%        -0.315000       3.000000       0.000000       1.000000   \n",
      "75%         1.642500       7.666667       0.000000       3.666667   \n",
      "max      9963.657500     232.666667     207.333333     385.333333   \n",
      "\n",
      "        blck_dat_Mean  unan_vce_Mean  unan_dat_Mean  plcd_vce_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.025777      27.784905       0.029810     144.881677   \n",
      "std          1.490255      38.358734       0.496970     158.267711   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000       5.000000       0.000000      38.333333   \n",
      "50%          0.000000      16.000000       0.000000     100.333333   \n",
      "75%          0.000000      36.000000       0.000000     198.666667   \n",
      "max        413.333333     848.666667      81.666667    2289.000000   \n",
      "\n",
      "        plcd_dat_Mean  recv_vce_Mean  recv_sms_Mean  comp_vce_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.870548      55.085152       0.047590     108.892535   \n",
      "std          9.053901      86.841198       2.127422     118.580079   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000       5.333333       0.000000      28.666667   \n",
      "50%          0.000000      26.666667       0.000000      75.666667   \n",
      "75%          0.000000      71.333333       0.000000     149.666667   \n",
      "max        733.666667    3369.333333     517.333333    1894.333333   \n",
      "\n",
      "        comp_dat_Mean  custcare_Mean  ccrndmou_Mean    cc_mou_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.774442       1.790838       4.666880       3.683328   \n",
      "std          8.129844       5.315570      12.761953      10.539981   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000       0.000000       0.000000       0.000000   \n",
      "50%          0.000000       0.000000       0.000000       0.000000   \n",
      "75%          0.000000       1.666667       4.000000       2.873333   \n",
      "max        559.333333     675.333333     861.333333     602.950000   \n",
      "\n",
      "        inonemin_Mean  threeway_Mean  mou_cvce_Mean  mou_cdat_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean        29.766550       0.284720     227.763520       1.844036   \n",
      "std         55.833753       1.092101     264.403894      23.733226   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          2.666667       0.000000      49.050833       0.000000   \n",
      "50%         12.333333       0.000000     146.203333       0.000000   \n",
      "75%         35.666667       0.333333     309.477500       0.000000   \n",
      "max       3086.666667      66.000000    4514.453333    3032.050000   \n",
      "\n",
      "        mou_rvce_Mean  owylis_vce_Mean  mouowylisv_Mean  iwylis_vce_Mean  \\\n",
      "count   100000.000000    100000.000000    100000.000000    100000.000000   \n",
      "unique            NaN              NaN              NaN              NaN   \n",
      "top               NaN              NaN              NaN              NaN   \n",
      "freq              NaN              NaN              NaN              NaN   \n",
      "mean       111.654562        24.753320        28.467865         7.894360   \n",
      "std        162.691285        34.414822        48.962903        16.145590   \n",
      "min          0.000000         0.000000         0.000000         0.000000   \n",
      "25%          7.650000         3.000000         2.376667         0.000000   \n",
      "50%         50.200000        13.000000        11.976667         2.000000   \n",
      "75%        149.452500        33.000000        34.174167         8.666667   \n",
      "max       3287.250000       644.333333      1802.706667       519.333333   \n",
      "\n",
      "        mouiwylisv_Mean  peak_vce_Mean  peak_dat_Mean  mou_peav_Mean  \\\n",
      "count     100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique              NaN            NaN            NaN            NaN   \n",
      "top                 NaN            NaN            NaN            NaN   \n",
      "freq                NaN            NaN            NaN            NaN   \n",
      "mean          18.193037      88.480285       0.358162     174.080788   \n",
      "std           41.421462     103.066103       4.065630     207.673553   \n",
      "min            0.000000       0.000000       0.000000       0.000000   \n",
      "25%            0.000000      21.666667       0.000000      37.642500   \n",
      "50%            3.210000      60.333333       0.000000     115.366667   \n",
      "75%           18.250000     118.666667       0.000000     233.223333   \n",
      "max         1703.536667    2090.666667     281.000000    4015.346667   \n",
      "\n",
      "        mou_pead_Mean   opk_vce_Mean   opk_dat_Mean  mou_opkv_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.707832      66.003498       0.416283     165.280703   \n",
      "std          8.410151      91.456885       4.652627     237.332918   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000      10.333333       0.000000      18.536667   \n",
      "50%          0.000000      34.333333       0.000000      75.841667   \n",
      "75%          0.000000      86.333333       0.000000     211.195000   \n",
      "max       1036.053333    1643.333333     309.666667    4337.893333   \n",
      "\n",
      "        mou_opkd_Mean  drop_blk_Mean   attempt_Mean  complete_Mean  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         1.135804      10.044298     145.752225     109.666977   \n",
      "std         17.768683      15.418406     159.348094     119.594305   \n",
      "min          0.000000       0.000000       0.000000       0.000000   \n",
      "25%          0.000000       1.666667      38.333333      28.666667   \n",
      "50%          0.000000       5.333333     101.000000      76.000000   \n",
      "75%          0.000000      12.333333     199.666667     150.666667   \n",
      "max       2922.043333     489.666667    2289.000000    1894.333333   \n",
      "\n",
      "        callfwdv_Mean  callwait_Mean          churn         months  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean         0.011700       1.782928       0.495620      18.833990   \n",
      "std          0.547470       5.353953       0.499983       9.655794   \n",
      "min          0.000000       0.000000       0.000000       6.000000   \n",
      "25%          0.000000       0.000000       0.000000      11.000000   \n",
      "50%          0.000000       0.333333       0.000000      16.000000   \n",
      "75%          0.000000       1.333333       1.000000      24.000000   \n",
      "max         81.333333     212.666667       1.000000      61.000000   \n",
      "\n",
      "             uniqsubs       actvsubs new_cell crclscod asl_flag  \\\n",
      "count   100000.000000  100000.000000   100000   100000   100000   \n",
      "unique            NaN            NaN        3       54        2   \n",
      "top               NaN            NaN        U       AA        N   \n",
      "freq              NaN            NaN    66914    36509    86064   \n",
      "mean         1.548140       1.358960      NaN      NaN      NaN   \n",
      "std          1.075255       0.655555      NaN      NaN      NaN   \n",
      "min          1.000000       0.000000      NaN      NaN      NaN   \n",
      "25%          1.000000       1.000000      NaN      NaN      NaN   \n",
      "50%          1.000000       1.000000      NaN      NaN      NaN   \n",
      "75%          2.000000       2.000000      NaN      NaN      NaN   \n",
      "max        196.000000      53.000000      NaN      NaN      NaN   \n",
      "\n",
      "             totcalls         totmou         totrev         adjrev  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean      2877.141930    7648.363833    1031.924988     960.111529   \n",
      "std       3790.863474    8666.558293     852.907511     840.170815   \n",
      "min          0.000000       0.000000       3.650000       2.400000   \n",
      "25%        889.000000    2529.000000     518.980000     452.180000   \n",
      "50%       1822.000000    5191.500000     804.530000     737.760000   \n",
      "75%       3492.000000    9776.000000    1263.767500    1188.175000   \n",
      "max      98874.000000  233419.096700   27321.500000   27071.300000   \n",
      "\n",
      "               adjmou         adjqty         avgrev         avgmou  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean      7546.314699    2836.366920      57.913832     483.726493   \n",
      "std       8594.889729    3756.513882      36.161292     438.485213   \n",
      "min          0.000000       0.000000       0.480000       0.000000   \n",
      "25%       2474.000000     868.000000      35.370000     176.140000   \n",
      "50%       5102.500000    1789.000000      49.890000     360.190000   \n",
      "75%       9661.000000    3442.000000      69.480000     655.670000   \n",
      "max     232855.100000   98705.000000     924.270000    7040.130000   \n",
      "\n",
      "               avgqty        avg3mou        avg3qty        avg3rev  \\\n",
      "count   100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "unique            NaN            NaN            NaN            NaN   \n",
      "top               NaN            NaN            NaN            NaN   \n",
      "freq              NaN            NaN            NaN            NaN   \n",
      "mean       173.554507     519.641210     180.337320      59.193330   \n",
      "std        167.824576     533.634073     192.725835      46.695429   \n",
      "min          0.000000       0.000000       0.000000       1.000000   \n",
      "25%         64.090000     152.000000      55.000000      33.000000   \n",
      "50%        127.500000     358.000000     125.000000      48.000000   \n",
      "75%        228.570000     711.000000     240.000000      71.000000   \n",
      "max       3017.110000    7716.000000    3909.000000    1593.000000   \n",
      "\n",
      "             avg6mou       avg6qty       avg6rev prizm_social_one  \\\n",
      "count   97161.000000  97161.000000  97161.000000            92612   \n",
      "unique           NaN           NaN           NaN                5   \n",
      "top              NaN           NaN           NaN                S   \n",
      "freq             NaN           NaN           NaN            32097   \n",
      "mean      509.634576    178.365157     58.683618              NaN   \n",
      "std       496.663453    182.724565     40.758577              NaN   \n",
      "min         0.000000      0.000000     -2.000000              NaN   \n",
      "25%       163.000000     59.000000     34.000000              NaN   \n",
      "50%       363.000000    127.000000     50.000000              NaN   \n",
      "75%       698.000000    237.000000     71.000000              NaN   \n",
      "max      7217.000000   3256.000000    866.000000              NaN   \n",
      "\n",
      "                      area dualband refurb_new     hnd_price        phones  \\\n",
      "count                99960    99999      99999  99153.000000  99999.000000   \n",
      "unique                  19        4          2           NaN           NaN   \n",
      "top     NEW YORK CITY AREA        Y          N           NaN           NaN   \n",
      "freq                 11098    72265      85593           NaN           NaN   \n",
      "mean                   NaN      NaN        NaN    101.875763      1.787118   \n",
      "std                    NaN      NaN        NaN     61.005671      1.313977   \n",
      "min                    NaN      NaN        NaN      9.989998      1.000000   \n",
      "25%                    NaN      NaN        NaN     29.989990      1.000000   \n",
      "50%                    NaN      NaN        NaN     99.989990      1.000000   \n",
      "75%                    NaN      NaN        NaN    149.989990      2.000000   \n",
      "max                    NaN      NaN        NaN    499.989990     28.000000   \n",
      "\n",
      "              models hnd_webcap         truck            rv ownrent  \\\n",
      "count   99999.000000      89811  98268.000000  98268.000000   66294   \n",
      "unique           NaN          3           NaN           NaN       2   \n",
      "top              NaN       WCMB           NaN           NaN       O   \n",
      "freq             NaN      75733           NaN           NaN   64284   \n",
      "mean        1.545825        NaN      0.188820      0.082580     NaN   \n",
      "std         0.898395        NaN      0.391368      0.275248     NaN   \n",
      "min         1.000000        NaN      0.000000      0.000000     NaN   \n",
      "25%         1.000000        NaN      0.000000      0.000000     NaN   \n",
      "50%         1.000000        NaN      0.000000      0.000000     NaN   \n",
      "75%         2.000000        NaN      0.000000      0.000000     NaN   \n",
      "max        16.000000        NaN      1.000000      1.000000     NaN   \n",
      "\n",
      "                 lor dwlltype marital        adults infobase        income  \\\n",
      "count   69810.000000    68091   98268  76981.000000    77921  74564.000000   \n",
      "unique           NaN        2       5           NaN        2           NaN   \n",
      "top              NaN        S       U           NaN        M           NaN   \n",
      "freq             NaN    48759   37333           NaN    77697           NaN   \n",
      "mean        6.177238      NaN     NaN      2.530326      NaN      5.783112   \n",
      "std         4.735267      NaN     NaN      1.452819      NaN      2.182132   \n",
      "min         0.000000      NaN     NaN      1.000000      NaN      1.000000   \n",
      "25%         2.000000      NaN     NaN      1.000000      NaN      4.000000   \n",
      "50%         5.000000      NaN     NaN      2.000000      NaN      6.000000   \n",
      "75%         9.000000      NaN     NaN      3.000000      NaN      7.000000   \n",
      "max        15.000000      NaN     NaN      6.000000      NaN      9.000000   \n",
      "\n",
      "            numbcars HHstatin dwllsize      forgntvl ethnic kid0_2 kid3_5  \\\n",
      "count   50634.000000    62077    61692  98268.000000  98268  98268  98268   \n",
      "unique           NaN        6       15           NaN     17      2      2   \n",
      "top              NaN        C        A           NaN      N      U      U   \n",
      "freq             NaN    39124    47205           NaN  33389  94256  93572   \n",
      "mean        1.567563      NaN      NaN      0.057974    NaN    NaN    NaN   \n",
      "std         0.625456      NaN      NaN      0.233696    NaN    NaN    NaN   \n",
      "min         1.000000      NaN      NaN      0.000000    NaN    NaN    NaN   \n",
      "25%         1.000000      NaN      NaN      0.000000    NaN    NaN    NaN   \n",
      "50%         1.000000      NaN      NaN      0.000000    NaN    NaN    NaN   \n",
      "75%         2.000000      NaN      NaN      0.000000    NaN    NaN    NaN   \n",
      "max         3.000000      NaN      NaN      1.000000    NaN    NaN    NaN   \n",
      "\n",
      "       kid6_10 kid11_15 kid16_17 creditcd       eqpdays   Customer_ID  \n",
      "count    98268    98268    98268    98268  99999.000000  1.000000e+05  \n",
      "unique       2        2        2        2           NaN           NaN  \n",
      "top          U        U        U        Y           NaN           NaN  \n",
      "freq     90195    89454    88304    67234           NaN           NaN  \n",
      "mean       NaN      NaN      NaN      NaN    391.932309  1.050000e+06  \n",
      "std        NaN      NaN      NaN      NaN    256.482193  2.886766e+04  \n",
      "min        NaN      NaN      NaN      NaN     -5.000000  1.000001e+06  \n",
      "25%        NaN      NaN      NaN      NaN    212.000000  1.025001e+06  \n",
      "50%        NaN      NaN      NaN      NaN    342.000000  1.050000e+06  \n",
      "75%        NaN      NaN      NaN      NaN    530.000000  1.075000e+06  \n",
      "max        NaN      NaN      NaN      NaN   1823.000000  1.100000e+06  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', 100):\n",
    "    print(churn_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECKING FOR NULL VALUES BEFORE IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rev_Mean              357\n",
      "mou_Mean              357\n",
      "totmrc_Mean           357\n",
      "da_Mean               357\n",
      "ovrmou_Mean           357\n",
      "ovrrev_Mean           357\n",
      "vceovr_Mean           357\n",
      "datovr_Mean           357\n",
      "roam_Mean             357\n",
      "change_mou            891\n",
      "change_rev            891\n",
      "avg6mou              2839\n",
      "avg6qty              2839\n",
      "avg6rev              2839\n",
      "prizm_social_one     7388\n",
      "area                   40\n",
      "dualband                1\n",
      "refurb_new              1\n",
      "hnd_price             847\n",
      "phones                  1\n",
      "models                  1\n",
      "hnd_webcap          10189\n",
      "truck                1732\n",
      "rv                   1732\n",
      "ownrent             33706\n",
      "lor                 30190\n",
      "dwlltype            31909\n",
      "marital              1732\n",
      "adults              23019\n",
      "infobase            22079\n",
      "income              25436\n",
      "numbcars            49366\n",
      "HHstatin            37923\n",
      "dwllsize            38308\n",
      "forgntvl             1732\n",
      "ethnic               1732\n",
      "kid0_2               1732\n",
      "kid3_5               1732\n",
      "kid6_10              1732\n",
      "kid11_15             1732\n",
      "kid16_17             1732\n",
      "creditcd             1732\n",
      "eqpdays                 1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_columns=churn_df.columns[churn_df.isnull().any()]\n",
    "print(churn_df[null_columns].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values for numerical variables with mean\n",
    "\n",
    "churn_df[\"rev_Mean\"].fillna(np.mean(churn_df[\"rev_Mean\"]),inplace=True)\n",
    "churn_df[\"mou_Mean\"].fillna(np.mean(churn_df[\"mou_Mean\"]),inplace=True)\n",
    "churn_df[\"totmrc_Mean\"].fillna(np.mean(churn_df[\"totmrc_Mean\"]),inplace=True)\n",
    "churn_df[\"da_Mean\"].fillna(np.mean(churn_df[\"da_Mean\"]),inplace=True)\n",
    "churn_df[\"ovrmou_Mean\"].fillna(np.mean(churn_df[\"ovrmou_Mean\"]),inplace=True)\n",
    "churn_df[\"ovrrev_Mean\"].fillna(np.mean(churn_df[\"ovrrev_Mean\"]),inplace=True)\n",
    "churn_df[\"vceovr_Mean\"].fillna(np.mean(churn_df[\"vceovr_Mean\"]),inplace=True)\n",
    "churn_df[\"datovr_Mean\"].fillna(np.mean(churn_df[\"datovr_Mean\"]),inplace=True)\n",
    "churn_df[\"roam_Mean\"].fillna(np.mean(churn_df[\"roam_Mean\"]),inplace=True)\n",
    "churn_df[\"change_mou\"].fillna(np.mean(churn_df[\"change_mou\"]),inplace=True)\n",
    "churn_df[\"change_rev\"].fillna(np.mean(churn_df[\"change_rev\"]),inplace=True)\n",
    "churn_df[\"avg6mou\"].fillna(np.mean(churn_df[\"avg6mou\"]),inplace=True)\n",
    "churn_df[\"avg6qty\"].fillna(np.mean(churn_df[\"avg6qty\"]),inplace=True)\n",
    "churn_df[\"avg6rev\"].fillna(np.mean(churn_df[\"avg6rev\"]),inplace=True)\n",
    "churn_df[\"hnd_price\"].fillna(np.mean(churn_df[\"hnd_price\"]),inplace=True)\n",
    "churn_df[\"phones\"].fillna(np.mean(churn_df[\"phones\"]),inplace=True)\n",
    "churn_df[\"models\"].fillna(np.mean(churn_df[\"models\"]),inplace=True)\n",
    "churn_df[\"lor\"].fillna(np.mean(churn_df[\"lor\"]),inplace=True)\n",
    "churn_df[\"eqpdays\"].fillna(np.mean(churn_df[\"eqpdays\"]),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values for categorical variables with mode\n",
    "\n",
    "churn_df[\"prizm_social_one\"].fillna(churn_df[\"prizm_social_one\"].mode()[0],inplace=True)\n",
    "churn_df[\"area\"].fillna(churn_df[\"area\"].mode()[0],inplace=True)\n",
    "churn_df[\"dualband\"].fillna(churn_df[\"dualband\"].mode()[0],inplace=True)\n",
    "churn_df[\"refurb_new\"].fillna(churn_df[\"refurb_new\"].mode()[0],inplace=True)\n",
    "churn_df[\"hnd_webcap\"].fillna(churn_df[\"hnd_webcap\"].mode()[0],inplace=True)\n",
    "churn_df[\"truck\"].fillna(churn_df[\"truck\"].mode()[0],inplace=True)\n",
    "churn_df[\"rv\"].fillna(churn_df[\"rv\"].mode()[0],inplace=True)\n",
    "churn_df[\"ownrent\"].fillna(churn_df[\"ownrent\"].mode()[0],inplace=True)\n",
    "churn_df[\"dwlltype\"].fillna(churn_df[\"dwlltype\"].mode()[0],inplace=True)\n",
    "churn_df[\"marital\"].fillna(churn_df[\"marital\"].mode()[0],inplace=True)\n",
    "churn_df[\"adults\"].fillna(churn_df[\"adults\"].mode()[0],inplace=True)\n",
    "churn_df[\"infobase\"].fillna(churn_df[\"infobase\"].mode()[0],inplace=True)\n",
    "churn_df[\"income\"].fillna(churn_df[\"income\"].mode()[0],inplace=True)\n",
    "churn_df[\"numbcars\"].fillna(churn_df[\"numbcars\"].mode()[0],inplace=True)\n",
    "churn_df[\"HHstatin\"].fillna(churn_df[\"HHstatin\"].mode()[0],inplace=True)\n",
    "churn_df[\"dwllsize\"].fillna(churn_df[\"dwllsize\"].mode()[0],inplace=True)\n",
    "churn_df[\"forgntvl\"].fillna(churn_df[\"forgntvl\"].mode()[0],inplace=True)\n",
    "churn_df[\"ethnic\"].fillna(churn_df[\"ethnic\"].mode()[0],inplace=True)\n",
    "churn_df[\"kid0_2\"].fillna(churn_df[\"kid0_2\"].mode()[0],inplace=True)\n",
    "churn_df[\"kid3_5\"].fillna(churn_df[\"kid3_5\"].mode()[0],inplace=True)\n",
    "churn_df[\"kid6_10\"].fillna(churn_df[\"kid6_10\"].mode()[0],inplace=True)\n",
    "churn_df[\"kid11_15\"].fillna(churn_df[\"kid11_15\"].mode()[0],inplace=True)\n",
    "churn_df[\"kid16_17\"].fillna(churn_df[\"kid16_17\"].mode()[0],inplace=True)\n",
    "churn_df[\"creditcd\"].fillna(churn_df[\"creditcd\"].mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_Mean            0\n",
       "mou_Mean            0\n",
       "totmrc_Mean         0\n",
       "da_Mean             0\n",
       "ovrmou_Mean         0\n",
       "ovrrev_Mean         0\n",
       "vceovr_Mean         0\n",
       "datovr_Mean         0\n",
       "roam_Mean           0\n",
       "change_mou          0\n",
       "change_rev          0\n",
       "drop_vce_Mean       0\n",
       "drop_dat_Mean       0\n",
       "blck_vce_Mean       0\n",
       "blck_dat_Mean       0\n",
       "unan_vce_Mean       0\n",
       "unan_dat_Mean       0\n",
       "plcd_vce_Mean       0\n",
       "plcd_dat_Mean       0\n",
       "recv_vce_Mean       0\n",
       "recv_sms_Mean       0\n",
       "comp_vce_Mean       0\n",
       "comp_dat_Mean       0\n",
       "custcare_Mean       0\n",
       "ccrndmou_Mean       0\n",
       "cc_mou_Mean         0\n",
       "inonemin_Mean       0\n",
       "threeway_Mean       0\n",
       "mou_cvce_Mean       0\n",
       "mou_cdat_Mean       0\n",
       "                   ..\n",
       "prizm_social_one    0\n",
       "area                0\n",
       "dualband            0\n",
       "refurb_new          0\n",
       "hnd_price           0\n",
       "phones              0\n",
       "models              0\n",
       "hnd_webcap          0\n",
       "truck               0\n",
       "rv                  0\n",
       "ownrent             0\n",
       "lor                 0\n",
       "dwlltype            0\n",
       "marital             0\n",
       "adults              0\n",
       "infobase            0\n",
       "income              0\n",
       "numbcars            0\n",
       "HHstatin            0\n",
       "dwllsize            0\n",
       "forgntvl            0\n",
       "ethnic              0\n",
       "kid0_2              0\n",
       "kid3_5              0\n",
       "kid6_10             0\n",
       "kid11_15            0\n",
       "kid16_17            0\n",
       "creditcd            0\n",
       "eqpdays             0\n",
       "Customer_ID         0\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking null values post imputation\n",
    "\n",
    "churn_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting categorical data into numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "labelencoder= LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables into numerical equivalents using LabelEncoder\n",
    "\n",
    "churn_df[\"new_cell\"] = labelencoder.fit_transform(churn_df['new_cell'])\n",
    "churn_df['crclscod'] = labelencoder.fit_transform(churn_df['crclscod'])\n",
    "churn_df[\"asl_flag\"] = labelencoder.fit_transform(churn_df['asl_flag'])\n",
    "churn_df[\"prizm_social_one\"] = labelencoder.fit_transform(churn_df['prizm_social_one'])\n",
    "churn_df[\"area\"] = labelencoder.fit_transform(churn_df['area'])\n",
    "churn_df[\"dualband\"] = labelencoder.fit_transform(churn_df['dualband'])\n",
    "churn_df[\"refurb_new\"] = labelencoder.fit_transform(churn_df['refurb_new'])\n",
    "churn_df[\"hnd_webcap\"] = labelencoder.fit_transform(churn_df['hnd_webcap'])\n",
    "churn_df[\"ownrent\"] = labelencoder.fit_transform(churn_df['ownrent'])\n",
    "churn_df[\"dwlltype\"] = labelencoder.fit_transform(churn_df['dwlltype'])\n",
    "churn_df[\"marital\"] = labelencoder.fit_transform(churn_df['marital'])\n",
    "churn_df[\"infobase\"] = labelencoder.fit_transform(churn_df['infobase'])\n",
    "churn_df[\"HHstatin\"] = labelencoder.fit_transform(churn_df['HHstatin'])\n",
    "churn_df[\"dwllsize\"] = labelencoder.fit_transform(churn_df['dwllsize'])\n",
    "churn_df[\"ethnic\"] = labelencoder.fit_transform(churn_df['ethnic'])\n",
    "churn_df[\"kid0_2\"] = labelencoder.fit_transform(churn_df['kid0_2'])\n",
    "churn_df[\"kid3_5\"] = labelencoder.fit_transform(churn_df['kid3_5'])\n",
    "churn_df[\"kid6_10\"] = labelencoder.fit_transform(churn_df['kid6_10'])\n",
    "churn_df[\"kid11_15\"] = labelencoder.fit_transform(churn_df['kid11_15'])\n",
    "churn_df[\"kid16_17\"] = labelencoder.fit_transform(churn_df['kid16_17'])\n",
    "churn_df[\"creditcd\"] = labelencoder.fit_transform(churn_df['creditcd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            rev_Mean       mou_Mean    totmrc_Mean        da_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean       58.719985     513.559937      46.179136       0.888828   \n",
      "std        46.208972     524.229868      23.581283       2.173729   \n",
      "min        -6.167500       0.000000     -26.915000       0.000000   \n",
      "25%        33.311875     151.500000      30.000000       0.000000   \n",
      "50%        48.377500     357.500000      44.990000       0.247500   \n",
      "75%        70.630000     701.250000      59.990000       0.888828   \n",
      "max      3843.262500   12206.750000     409.990000     159.390000   \n",
      "\n",
      "         ovrmou_Mean    ovrrev_Mean    vceovr_Mean    datovr_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean       41.072247      13.559560      13.295062       0.261318   \n",
      "std        97.122320      30.446392      30.002391       3.120946   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         3.000000       1.050000       0.700000       0.000000   \n",
      "75%        42.000000      14.350000      13.950000       0.000000   \n",
      "max      4320.750000    1102.400000     896.087500     423.540000   \n",
      "\n",
      "           roam_Mean     change_mou     change_rev  drop_vce_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        1.286405     -13.933818      -1.021067       5.955085   \n",
      "std        14.685090     274.854774      50.138337       8.954715   \n",
      "min         0.000000   -3875.000000   -1107.740000       0.000000   \n",
      "25%         0.000000     -86.000000      -7.203125       0.666667   \n",
      "50%         0.000000      -7.000000      -0.322500       3.000000   \n",
      "75%         0.257500      61.750000       1.545000       7.666667   \n",
      "max      3685.200000   31219.250000    9963.657500     232.666667   \n",
      "\n",
      "       drop_dat_Mean  blck_vce_Mean  blck_dat_Mean  unan_vce_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.040520       4.022917       0.025777      27.784905   \n",
      "std         0.877136      10.672202       1.490255      38.358734   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       5.000000   \n",
      "50%         0.000000       1.000000       0.000000      16.000000   \n",
      "75%         0.000000       3.666667       0.000000      36.000000   \n",
      "max       207.333333     385.333333     413.333333     848.666667   \n",
      "\n",
      "       unan_dat_Mean  plcd_vce_Mean  plcd_dat_Mean  recv_vce_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.029810     144.881677       0.870548      55.085152   \n",
      "std         0.496970     158.267711       9.053901      86.841198   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000      38.333333       0.000000       5.333333   \n",
      "50%         0.000000     100.333333       0.000000      26.666667   \n",
      "75%         0.000000     198.666667       0.000000      71.333333   \n",
      "max        81.666667    2289.000000     733.666667    3369.333333   \n",
      "\n",
      "       recv_sms_Mean  comp_vce_Mean  comp_dat_Mean  custcare_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.047590     108.892535       0.774442       1.790838   \n",
      "std         2.127422     118.580079       8.129844       5.315570   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000      28.666667       0.000000       0.000000   \n",
      "50%         0.000000      75.666667       0.000000       0.000000   \n",
      "75%         0.000000     149.666667       0.000000       1.666667   \n",
      "max       517.333333    1894.333333     559.333333     675.333333   \n",
      "\n",
      "       ccrndmou_Mean    cc_mou_Mean  inonemin_Mean  threeway_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        4.666880       3.683328      29.766550       0.284720   \n",
      "std        12.761953      10.539981      55.833753       1.092101   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       2.666667       0.000000   \n",
      "50%         0.000000       0.000000      12.333333       0.000000   \n",
      "75%         4.000000       2.873333      35.666667       0.333333   \n",
      "max       861.333333     602.950000    3086.666667      66.000000   \n",
      "\n",
      "       mou_cvce_Mean  mou_cdat_Mean  mou_rvce_Mean  owylis_vce_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000    100000.000000   \n",
      "mean      227.763520       1.844036     111.654562        24.753320   \n",
      "std       264.403894      23.733226     162.691285        34.414822   \n",
      "min         0.000000       0.000000       0.000000         0.000000   \n",
      "25%        49.050833       0.000000       7.650000         3.000000   \n",
      "50%       146.203333       0.000000      50.200000        13.000000   \n",
      "75%       309.477500       0.000000     149.452500        33.000000   \n",
      "max      4514.453333    3032.050000    3287.250000       644.333333   \n",
      "\n",
      "       mouowylisv_Mean  iwylis_vce_Mean  mouiwylisv_Mean  peak_vce_Mean  \\\n",
      "count    100000.000000    100000.000000    100000.000000  100000.000000   \n",
      "mean         28.467865         7.894360        18.193037      88.480285   \n",
      "std          48.962903        16.145590        41.421462     103.066103   \n",
      "min           0.000000         0.000000         0.000000       0.000000   \n",
      "25%           2.376667         0.000000         0.000000      21.666667   \n",
      "50%          11.976667         2.000000         3.210000      60.333333   \n",
      "75%          34.174167         8.666667        18.250000     118.666667   \n",
      "max        1802.706667       519.333333      1703.536667    2090.666667   \n",
      "\n",
      "       peak_dat_Mean  mou_peav_Mean  mou_pead_Mean   opk_vce_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.358162     174.080788       0.707832      66.003498   \n",
      "std         4.065630     207.673553       8.410151      91.456885   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000      37.642500       0.000000      10.333333   \n",
      "50%         0.000000     115.366667       0.000000      34.333333   \n",
      "75%         0.000000     233.223333       0.000000      86.333333   \n",
      "max       281.000000    4015.346667    1036.053333    1643.333333   \n",
      "\n",
      "        opk_dat_Mean  mou_opkv_Mean  mou_opkd_Mean  drop_blk_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.416283     165.280703       1.135804      10.044298   \n",
      "std         4.652627     237.332918      17.768683      15.418406   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000      18.536667       0.000000       1.666667   \n",
      "50%         0.000000      75.841667       0.000000       5.333333   \n",
      "75%         0.000000     211.195000       0.000000      12.333333   \n",
      "max       309.666667    4337.893333    2922.043333     489.666667   \n",
      "\n",
      "        attempt_Mean  complete_Mean  callfwdv_Mean  callwait_Mean  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean      145.752225     109.666977       0.011700       1.782928   \n",
      "std       159.348094     119.594305       0.547470       5.353953   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%        38.333333      28.666667       0.000000       0.000000   \n",
      "50%       101.000000      76.000000       0.000000       0.333333   \n",
      "75%       199.666667     150.666667       0.000000       1.333333   \n",
      "max      2289.000000    1894.333333      81.333333     212.666667   \n",
      "\n",
      "               churn         months       uniqsubs       actvsubs  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.495620      18.833990       1.548140       1.358960   \n",
      "std         0.499983       9.655794       1.075255       0.655555   \n",
      "min         0.000000       6.000000       1.000000       0.000000   \n",
      "25%         0.000000      11.000000       1.000000       1.000000   \n",
      "50%         0.000000      16.000000       1.000000       1.000000   \n",
      "75%         1.000000      24.000000       2.000000       2.000000   \n",
      "max         1.000000      61.000000     196.000000      53.000000   \n",
      "\n",
      "            new_cell       crclscod       asl_flag       totcalls  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        1.055160       8.200890       0.139360    2877.141930   \n",
      "std         0.572556      11.353526       0.346324    3790.863474   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         1.000000       3.000000       0.000000     889.000000   \n",
      "50%         1.000000       3.000000       0.000000    1822.000000   \n",
      "75%         1.000000      10.000000       0.000000    3492.000000   \n",
      "max         2.000000      53.000000       1.000000   98874.000000   \n",
      "\n",
      "              totmou         totrev         adjrev         adjmou  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean     7648.363833    1031.924988     960.111529    7546.314699   \n",
      "std      8666.558293     852.907511     840.170815    8594.889729   \n",
      "min         0.000000       3.650000       2.400000       0.000000   \n",
      "25%      2529.000000     518.980000     452.180000    2474.000000   \n",
      "50%      5191.500000     804.530000     737.760000    5102.500000   \n",
      "75%      9776.000000    1263.767500    1188.175000    9661.000000   \n",
      "max    233419.096700   27321.500000   27071.300000  232855.100000   \n",
      "\n",
      "              adjqty         avgrev         avgmou         avgqty  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean     2836.366920      57.913832     483.726493     173.554507   \n",
      "std      3756.513882      36.161292     438.485213     167.824576   \n",
      "min         0.000000       0.480000       0.000000       0.000000   \n",
      "25%       868.000000      35.370000     176.140000      64.090000   \n",
      "50%      1789.000000      49.890000     360.190000     127.500000   \n",
      "75%      3442.000000      69.480000     655.670000     228.570000   \n",
      "max     98705.000000     924.270000    7040.130000    3017.110000   \n",
      "\n",
      "             avg3mou        avg3qty        avg3rev        avg6mou  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean      519.641210     180.337320      59.193330     509.634576   \n",
      "std       533.634073     192.725835      46.695429     489.562483   \n",
      "min         0.000000       0.000000       1.000000       0.000000   \n",
      "25%       152.000000      55.000000      33.000000     168.000000   \n",
      "50%       358.000000     125.000000      48.000000     377.000000   \n",
      "75%       711.000000     240.000000      71.000000     683.000000   \n",
      "max      7716.000000    3909.000000    1593.000000    7217.000000   \n",
      "\n",
      "             avg6qty        avg6rev  prizm_social_one           area  \\\n",
      "count  100000.000000  100000.000000      100000.00000  100000.000000   \n",
      "mean      178.365157      58.683618           2.23284       8.470250   \n",
      "std       180.112088      40.175838           1.33037       5.187744   \n",
      "min         0.000000      -2.000000           0.00000       0.000000   \n",
      "25%        61.000000      35.000000           2.00000       4.000000   \n",
      "50%       132.000000      50.000000           2.00000       9.000000   \n",
      "75%       232.000000      70.000000           3.00000      12.000000   \n",
      "max      3256.000000     866.000000           4.00000      18.000000   \n",
      "\n",
      "            dualband     refurb_new      hnd_price         phones  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        2.215580       0.144060     101.875763       1.787118   \n",
      "std         1.283434       0.351152      60.746760       1.313971   \n",
      "min         0.000000       0.000000       9.989998       1.000000   \n",
      "25%         1.000000       0.000000      29.989990       1.000000   \n",
      "50%         3.000000       0.000000      99.989990       1.000000   \n",
      "75%         3.000000       0.000000     149.989990       2.000000   \n",
      "max         3.000000       1.000000     499.989990      28.000000   \n",
      "\n",
      "              models     hnd_webcap          truck             rv  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        1.545825       1.856870       0.185550       0.081150   \n",
      "std         0.898391       0.356854       0.388745       0.273067   \n",
      "min         1.000000       0.000000       0.000000       0.000000   \n",
      "25%         1.000000       2.000000       0.000000       0.000000   \n",
      "50%         1.000000       2.000000       0.000000       0.000000   \n",
      "75%         2.000000       2.000000       0.000000       0.000000   \n",
      "max        16.000000       2.000000       1.000000       1.000000   \n",
      "\n",
      "             ownrent            lor       dwlltype        marital  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        0.020100       6.177238       0.806680       2.783610   \n",
      "std         0.140343       3.956420       0.394904       1.183892   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       3.000000       1.000000       2.000000   \n",
      "50%         0.000000       6.177238       1.000000       3.000000   \n",
      "75%         0.000000       7.000000       1.000000       4.000000   \n",
      "max         1.000000      15.000000       1.000000       4.000000   \n",
      "\n",
      "              adults       infobase         income       numbcars  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        2.408250       0.002240       5.838280       1.287380   \n",
      "std         1.294086       0.047276       1.886643       0.527821   \n",
      "min         1.000000       0.000000       1.000000       1.000000   \n",
      "25%         2.000000       0.000000       5.000000       1.000000   \n",
      "50%         2.000000       0.000000       6.000000       1.000000   \n",
      "75%         3.000000       0.000000       7.000000       2.000000   \n",
      "max         6.000000       1.000000       9.000000       3.000000   \n",
      "\n",
      "            HHstatin       dwllsize       forgntvl         ethnic  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        2.280900       0.801300       0.056970       9.110500   \n",
      "std         1.143008       2.672708       0.231786       3.700514   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         2.000000       0.000000       0.000000       6.000000   \n",
      "50%         2.000000       0.000000       0.000000       9.000000   \n",
      "75%         2.000000       0.000000       0.000000      13.000000   \n",
      "max         5.000000      14.000000       1.000000      16.000000   \n",
      "\n",
      "              kid0_2         kid3_5        kid6_10      kid11_15  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.00000   \n",
      "mean        0.040120       0.046960       0.080730       0.08814   \n",
      "std         0.196242       0.211554       0.272421       0.28350   \n",
      "min         0.000000       0.000000       0.000000       0.00000   \n",
      "25%         0.000000       0.000000       0.000000       0.00000   \n",
      "50%         0.000000       0.000000       0.000000       0.00000   \n",
      "75%         0.000000       0.000000       0.000000       0.00000   \n",
      "max         1.000000       1.000000       1.000000       1.00000   \n",
      "\n",
      "            kid16_17       creditcd        eqpdays   Customer_ID  \n",
      "count  100000.000000  100000.000000  100000.000000  1.000000e+05  \n",
      "mean        0.099640       0.689660     391.932309  1.050000e+06  \n",
      "std         0.299521       0.462635     256.480910  2.886766e+04  \n",
      "min         0.000000       0.000000      -5.000000  1.000001e+06  \n",
      "25%         0.000000       0.000000     212.000000  1.025001e+06  \n",
      "50%         0.000000       1.000000     342.000000  1.050000e+06  \n",
      "75%         0.000000       1.000000     530.000000  1.075000e+06  \n",
      "max         1.000000       1.000000    1823.000000  1.100000e+06  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_columns', 100):\n",
    "    print(churn_df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_df_final = churn_df.drop(['Customer_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributing the dataset into two components X and Y \n",
    "\n",
    "X = churn_df_final.drop([\"churn\"], axis = 1)\n",
    "y = churn_df_final[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 98)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of X (input features)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of y (output features)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the X and Y into the \n",
    "# Training set and Testing set \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing preprocessing part \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "sc = StandardScaler() \n",
    "  \n",
    "X_train = sc.fit_transform(X_train) \n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA function on training \n",
    "# and testing set of X component \n",
    "from sklearn.decomposition import PCA \n",
    "  \n",
    "pca = PCA(n_components = 2) \n",
    "  \n",
    "X_train = pca.fit_transform(X_train) \n",
    "X_test = pca.transform(X_test) \n",
    "  \n",
    "explained_variance = pca.explained_variance_ratio_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting Logistic Regression into the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=100, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression   \n",
    "  \n",
    "classifier = LogisticRegression(random_state = 100) \n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set result using  \n",
    "# predict function under LogisticRegression  \n",
    "y_pred = classifier.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5159"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4708 5337]\n",
      " [4345 5610]]\n"
     ]
    }
   ],
   "source": [
    "# making confusion matrix between \n",
    "#  test set of Y and predicted value. \n",
    "from sklearn.metrics import confusion_matrix \n",
    "  \n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Training set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from matplotlib.colors import ListedColormap \\n  \\nX_set, y_set = X_train, y_train \\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \\n                     stop = X_set[:, 0].max() + 1, step = 0.01), \\n                     np.arange(start = X_set[:, 1].min() - 1, \\n                     stop = X_set[:, 1].max() + 1, step = 0.01)) \\n  \\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \\n             X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \\n             cmap = ListedColormap(('yellow', 'white', 'aquamarine'))) \\n  \\nplt.xlim(X1.min(), X1.max()) \\nplt.ylim(X2.min(), X2.max()) \\n  \\nfor i, j in enumerate(np.unique(y_set)): \\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j) \\n  \\nplt.title('Logistic Regression (Training set)') \\nplt.xlabel('PC1') # for Xlabel \\nplt.ylabel('PC2') # for Ylabel \\nplt.legend() # to show legend \\n  \\n# show scatter plot \\nplt.show() \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the training set \n",
    "# result through scatter plot  \n",
    "\"\"\"from matplotlib.colors import ListedColormap \n",
    "  \n",
    "X_set, y_set = X_train, y_train \n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \n",
    "                     stop = X_set[:, 0].max() + 1, step = 0.01), \n",
    "                     np.arange(start = X_set[:, 1].min() - 1, \n",
    "                     stop = X_set[:, 1].max() + 1, step = 0.01)) \n",
    "  \n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \n",
    "             X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \n",
    "             cmap = ListedColormap(('yellow', 'white', 'aquamarine'))) \n",
    "  \n",
    "plt.xlim(X1.min(), X1.max()) \n",
    "plt.ylim(X2.min(), X2.max()) \n",
    "  \n",
    "for i, j in enumerate(np.unique(y_set)): \n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j) \n",
    "  \n",
    "plt.title('Logistic Regression (Training set)') \n",
    "plt.xlabel('PC1') # for Xlabel \n",
    "plt.ylabel('PC2') # for Ylabel \n",
    "plt.legend() # to show legend \n",
    "  \n",
    "# show scatter plot \n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Testing set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from matplotlib.colors import ListedColormap \\n  \\nX_set, y_set = X_test, y_test \\n  \\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \\n                     stop = X_set[:, 0].max() + 1, step = 0.01), \\n                     np.arange(start = X_set[:, 1].min() - 1, \\n                     stop = X_set[:, 1].max() + 1, step = 0.01)) \\n  \\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \\n             X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \\n             cmap = ListedColormap(('yellow', 'white', 'aquamarine')))  \\n  \\nplt.xlim(X1.min(), X1.max()) \\nplt.ylim(X2.min(), X2.max()) \\n  \\nfor i, j in enumerate(np.unique(y_set)): \\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j) \\n  \\n# title for scatter plot \\nplt.title('Logistic Regression (Test set)')  \\nplt.xlabel('PC1') # for Xlabel \\nplt.ylabel('PC2') # for Ylabel \\nplt.legend() \\n  \\n# show scatter plot \\nplt.show() \""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualising the Test set results through scatter plot \n",
    "\"\"\"from matplotlib.colors import ListedColormap \n",
    "  \n",
    "X_set, y_set = X_test, y_test \n",
    "  \n",
    "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, \n",
    "                     stop = X_set[:, 0].max() + 1, step = 0.01), \n",
    "                     np.arange(start = X_set[:, 1].min() - 1, \n",
    "                     stop = X_set[:, 1].max() + 1, step = 0.01)) \n",
    "  \n",
    "plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), \n",
    "             X2.ravel()]).T).reshape(X1.shape), alpha = 0.75, \n",
    "             cmap = ListedColormap(('yellow', 'white', 'aquamarine')))  \n",
    "  \n",
    "plt.xlim(X1.min(), X1.max()) \n",
    "plt.ylim(X2.min(), X2.max()) \n",
    "  \n",
    "for i, j in enumerate(np.unique(y_set)): \n",
    "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], \n",
    "                c = ListedColormap(('red', 'green', 'blue'))(i), label = j) \n",
    "  \n",
    "# title for scatter plot \n",
    "plt.title('Logistic Regression (Test set)')  \n",
    "plt.xlabel('PC1') # for Xlabel \n",
    "plt.ylabel('PC2') # for Ylabel \n",
    "plt.legend() \n",
    "  \n",
    "# show scatter plot \n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random FOrest with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [200,300,400,500,600,700,800,900,1000]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5,6,7,8,9,10,11,12,13,14,15]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 65.4min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 115.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators='warn',\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_sc...\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                                      13, 14, 15],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 300, 400, 500,\n",
       "                                                         600, 700, 800, 900,\n",
       "                                                         1000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=10, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=2, random_state = 10, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 900,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 5,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually provide the best parameters to model for training\n",
    "rf1 = RandomForestClassifier(**{'n_estimators': 900,\n",
    " 'min_samples_split': 5,\n",
    " 'min_samples_leaf': 1,\n",
    " 'max_features': 'sqrt',\n",
    " 'max_depth': 5,\n",
    " 'bootstrap': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=5, max_features='sqrt', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=5,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=900,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = rf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with RandomizedSearcCV Accuracy:  0.5298\n"
     ]
    }
   ],
   "source": [
    "print(\"Random Forest with RandomizedSearcCV Accuracy: \",accuracy_score(y_test,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[5869 4176]\n",
      " [5228 4727]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix: \")\n",
    "cm = confusion_matrix(y_test, ypred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model no training data\n",
    "xgbmodel = XGBClassifier()\n",
    "xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = xgbmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = xgbmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for XGBoost model: 52.62\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for XGBoost model: %.2f\" % (accuracy_score(y_test, ypred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
